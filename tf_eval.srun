#!/bin/bash
#SBATCH --job-name=tfeval
#SBATCH --output=slurm_logs/tf_eval.out
#SBATCH --error=slurm_logs/tf_eval.err
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --nodes=1-1
#SBATCH --gres=gpu:tesla:4
#SBATCH --partition=kamiak
#SBATCH --time=0-04:00:00
#SBATCH --mem=30G

. config.py

#
# ---
#

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
module load cuda/8.0.44 cudnn/6.0_cuda8.0 python3/3.5.0
data="$remotedir"

function clean_up 
{
    rmworkspace -a -f --name="$SCRATCHDIR"
    exit
}

#Create a scratch workspace
SCRATCHDIR="$(mkworkspace -q -t 7-00:00 -b /local)" # 7 days
trap 'clean_up' EXIT

echo "Scratch space: $SCRATCHDIR"
echo "SLURM_CPUS_PER_TASK: $SLURM_CPUS_PER_TASK"
echo "SLURM_JOB_GPUS: $SLURM_JOB_GPUS"

echo "Getting data: started"
cd "$SCRATCHDIR"

echo " - dataset"
# The files must be datasets/YourDataSet/tftrain.record, etc. since the config
# file specifies that directory
mkdir -p "$datasetFolder"
cp -a "$data/$datasetTFtrain" \
      "$data/$datasetTFvalid" \
      "$data/$datasetTFtest" \
      "$data/$datasetTFlabels" \
      "$data/$datasetTFconfig" \
      "$data/$datasetFolder/${TFArch}"_model.ckpt* \
      "$datasetFolder"
echo " - TF models"
cp -ra "$data/models" .
echo "Getting data: done"

echo "Making sure TensorFlow installed: starting"
pip install --user tensorflow-gpu pillow lxml jupyter matplotlib
echo "Making sure TensorFlow installed: done"

echo "Evaluating network: started"
mkdir -p "$data/${datasetTFevallogs}_$TFArch" # log dir, rsync this to view with TensorBoard
cd models/research
export PYTHONPATH="$PYTHONPATH:$(pwd):$(pwd)/slim"
python3 object_detection/eval.py \
    --checkpoint_dir="$data/${datasetTFtrainlogs}_$TFArch" \
    --eval_dir="$data/${datasetTFevallogs}_$TFArch" \
    --pipeline_config_path="../../$datasetTFconfig"
echo "Evaluating network: done"

echo "Deleting workspace: started"
clean_up
echo "Deleting workspace: done"
