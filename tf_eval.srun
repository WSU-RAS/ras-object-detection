#!/bin/bash
#SBATCH --job-name=tfeval
#SBATCH --output=slurm_logs/tf_eval.out
#SBATCH --error=slurm_logs/tf_eval.err
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --nodes=1-1
#SBATCH --gres=gpu:tesla:4
#SBATCH --partition=kamiak
#SBATCH --time=0-02:00:00
#SBATCH --mem=30G

. config.py

#
# ---
#

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
module load git/2.6.3 gcc/5.2.0 cuda/8.0.44 cudnn/5.1_cuda8.0 python3
data="$remotedir"

function clean_up 
{
    rmworkspace -a -f --name="$SCRATCHDIR"
    exit
}

#Create a scratch workspace
SCRATCHDIR="$(mkworkspace -q -t 7-00:00 -b /local)" # 7 days
trap 'clean_up' EXIT

echo "Scratch space: $SCRATCHDIR"
echo "SLURM_CPUS_PER_TASK: $SLURM_CPUS_PER_TASK"
echo "SLURM_JOB_GPUS: $SLURM_JOB_GPUS"

echo "Getting data: started"
cd "$SCRATCHDIR"

echo " - dataset"
# The files must be datasets/YourDataSet/tftrain.record, etc. since the config
# file specifies that directory
mkdir -p "$datasetFolder"
cp -a "$data/$datasetTFtrain" \
      "$data/$datasetTFvalid" \
      "$data/$datasetTFtest" \
      "$data/$datasetTFlabels" \
      "$data/$datasetTFconfig" \
      "$datasetFolder"

echo " - TF models"
cp -ra "$data/models" .
cd models/research/
protoc object_detection/protos/*.proto --python_out=.
cd "$SCRATCHDIR"

echo " - code"
echo "Getting data: done"

echo "Making sure TensorFlow installed: starting"
# Tensorflow 1.3 from pip requires cuDNN 6 but we only have 5.1, so use TF 1.2.1
pip install --user tensorflow-gpu==1.2.1 pillow lxml jupyter matplotlib
echo "Making sure TensorFlow installed: done"

echo "Evaluating network: started"
mkdir -p "$data/$datasetTFevallogs" # log dir, rsync this to view with TensorBoard
cd models/research
export PYTHONPATH="$PYTHONPATH:$(pwd):$(pwd)/slim"
python3 object_detection/eval.py \
    --checkpoint_dir="$data/$datasetTFtrainlogs" \
    --eval_dir="$data/$datasetTFevallogs" \
    --pipeline_config_path="../../$datasetTFconfig"
echo "Evaluating network: done"

echo "Deleting workspace: started"
clean_up
echo "Deleting workspace: done"
